<?xml version="1.0" encoding="UTF-8"?>
<doc>


    <text id="H01-1001">
        <title>Activity detection for information access to oral communication</title>
        <abstract>
            <entity id="H01-1001.1">Oral communication</entity> is ubiquitous and carries important information yet it
            is also time consuming to document. Given the development of <entity id="H01-1001.2">storage media and
            networks
        </entity> one could just record and store a <entity id="H01-1001.3">conversation</entity> for documentation. The
            question is, however, how an interesting information piece would be found in a <entity id="H01-1001.4">large
            database
        </entity> . Traditional <entity id="H01-1001.5">information retrieval techniques</entity> use a <entity
                id="H01-1001.6">histogram
        </entity> of <entity id="H01-1001.7">keywords</entity> as the <entity id="H01-1001.8">document representation
        </entity> but <entity id="H01-1001.9">oral communication</entity> may offer additional <entity id="H01-1001.10">
            indices
        </entity> such as the time and place of the rejoinder and the attendance. An alternative <entity
                id="H01-1001.11">index
        </entity> could be the activity such as discussing, planning, informing, story-telling, etc. This paper
            addresses the problem of the <entity id="H01-1001.12">automatic detection</entity> of those activities in
            meeting situation and everyday rejoinders. Several extensions of this basic idea are being discussed and/or
            evaluated: Similar to activities one can define subsets of larger <entity id="H01-1001.13">database</entity> and
            detect those automatically which is shown on a large <entity id="H01-1001.14">database</entity> of <entity
                id="H01-1001.15">TV shows
        </entity> . <entity id="H01-1001.16">Emotions</entity> and other <entity id="H01-1001.17">indices</entity> such
            as the <entity id="H01-1001.18">dominance distribution of speakers</entity> might be available on the <entity
                id="H01-1001.19">surface
        </entity> and could be used directly. Despite the small size of the <entity id="H01-1001.20">databases</entity> used
            some results about the effectiveness of these <entity id="H01-1001.21">indices</entity> can be obtained.
        </abstract>


    </text>

    <text id="H01-1017">
        <title>Dialogue Interaction with the DARPA Communicator Infrastructure: The Development of Useful Software
        </title>
        <abstract>To support engaging human users in robust, <entity id="H01-1017.1">mixed-initiative speech dialogue
            interactions
        </entity> which reach beyond current capabilities in <entity id="H01-1017.2">dialogue systems</entity> , the <entity
                id="H01-1017.3">DARPA Communicator program
        </entity> [1] is funding the development of a <entity id="H01-1017.4">distributed message-passing
            infrastructure
        </entity> for <entity id="H01-1017.5">dialogue systems</entity> which all <entity id="H01-1017.6">Communicator
        </entity> participants are using. In this presentation, we describe the features of and <entity id="H01-1017.7">
            requirements
        </entity> for a genuinely useful <entity id="H01-1017.8">software infrastructure</entity> for this purpose.
        </abstract>


    </text>

    <text id="H01-1041">
        <title>Interlingua-Based Broad-Coverage Korean-to-English Translation in CCLING</title>
        <abstract>At MIT Lincoln Laboratory, we have been developing a <entity id="H01-1041.1">Korean-to-English machine
            translation system
        </entity>
            <entity id="H01-1041.2">CCLINC (Common Coalition Language System at Lincoln Laboratory)</entity> . The <entity
                    id="H01-1041.3">CCLINC Korean-to-English translation system
            </entity> consists of two <entity id="H01-1041.4">core modules</entity> , <entity id="H01-1041.5">language
                understanding and generation modules
            </entity> mediated by a <entity id="H01-1041.6">language neutral meaning representation</entity> called a <entity
                    id="H01-1041.7">semantic frame
            </entity> . The key features of the system include: (i) Robust efficient <entity id="H01-1041.8">parsing
            </entity> of <entity id="H01-1041.9">Korean</entity> (a <entity id="H01-1041.10">verb final language
            </entity> with <entity id="H01-1041.11">overt case markers</entity> , relatively <entity id="H01-1041.12">
                free word order
            </entity> , and frequent omissions of <entity id="H01-1041.13">arguments</entity> ). (ii) High quality <entity
                    id="H01-1041.14">translation
            </entity> via <entity id="H01-1041.15">word sense disambiguation</entity> and accurate <entity
                    id="H01-1041.16">word order generation
            </entity> of the <entity id="H01-1041.17">target language</entity> . (iii) <entity id="H01-1041.18">Rapid
                system development
            </entity> and porting to new <entity id="H01-1041.19">domains</entity> via <entity id="H01-1041.20">
                knowledge-based automated acquisition of grammars
            </entity> . Having been trained on <entity id="H01-1041.21">Korean newspaper articles</entity> on missiles
            and chemical biological warfare, the system produces the <entity id="H01-1041.22">translation output
            </entity> sufficient for content understanding of the <entity id="H01-1041.23">original document</entity> .
        </abstract>


    </text>

    <text id="H01-1042">
        <title>Is That Your Final Answer?</title>
        <abstract>The purpose of this research is to test the efficacy of applying <entity id="H01-1042.1">automated
            evaluation techniques
        </entity> , originally devised for the evaluation of <entity id="H01-1042.2">human language learners</entity> ,
            to the <entity id="H01-1042.3">output</entity> of <entity id="H01-1042.4">machine translation (MT) systems
            </entity> . We believe that these <entity id="H01-1042.5">evaluation techniques</entity> will provide
            information about both the <entity id="H01-1042.6">human language learning process</entity> , the <entity
                    id="H01-1042.7">translation process
            </entity> and the <entity id="H01-1042.8">development</entity> of <entity id="H01-1042.9">machine
                translation systems
            </entity> . This, the first experiment in a series of experiments, looks at the <entity id="H01-1042.10">
                intelligibility
            </entity> of <entity id="H01-1042.11">MT output</entity> . A <entity id="H01-1042.12">language learning
                experiment
            </entity> showed that <entity id="H01-1042.13">assessors</entity> can differentiate <entity
                    id="H01-1042.14">native from non-native language essays
            </entity> in less than 100 <entity id="H01-1042.15">words</entity> . Even more illuminating was the factors
            on which the <entity id="H01-1042.16">assessors</entity> made their decisions. We tested this to see if
            similar criteria could be elicited from duplicating the experiment using <entity id="H01-1042.17">machine
                translation output
            </entity> . Subjects were given a set of up to six extracts of <entity id="H01-1042.18">translated newswire
                text
            </entity> . Some of the extracts were <entity id="H01-1042.19">expert human translations</entity> , others
            were <entity id="H01-1042.20">machine translation outputs</entity> . The subjects were given three minutes
            per extract to determine whether they believed the sample output to be an <entity id="H01-1042.21">expert
                human translation
            </entity> or a <entity id="H01-1042.22">machine translation</entity> . Additionally, they were asked to mark
            the <entity id="H01-1042.23">word</entity> at which they made this decision. The results of this experiment,
            along with a preliminary analysis of the factors involved in the decision making process will be presented
            here.
        </abstract>


    </text>

    <text id="H01-1049">
        <title>Listen-Communicate-Show (LCS): Spoken Language Command of Agent-based Remote Information Access</title>
        <abstract>
            <entity id="H01-1049.1">Listen-Communicate-Show (LCS)</entity> is a new paradigm for <entity
                id="H01-1049.2">human interaction with data sources
        </entity> . We integrate a <entity id="H01-1049.3">spoken language understanding system</entity> with <entity
                id="H01-1049.4">intelligent mobile agents
        </entity> that mediate between <entity id="H01-1049.5">users</entity> and <entity id="H01-1049.6">information
            sources
        </entity> . We have built and will demonstrate an application of this approach called <entity id="H01-1049.7">
            LCS-Marine
        </entity> . Using <entity id="H01-1049.8">LCS-Marine</entity> , tactical personnel can converse with their
            logistics system to place a supply or information request. The request is passed to a <entity
                id="H01-1049.9">mobile, intelligent agent
        </entity> for execution at the appropriate <entity id="H01-1049.10">database</entity> . <entity
                id="H01-1049.11">Requestors
        </entity> can also instruct the system to notify them when the status of a <entity id="H01-1049.12">request
        </entity> changes or when a <entity id="H01-1049.13">request</entity> is complete. We have demonstrated this
            capability in several field exercises with the Marines and are currently developing applications of this
            technology in <entity id="H01-1049.14">new domains</entity> .
        </abstract>


    </text>

    <text id="H01-1058">
        <title>On Combining Language Models : Oracle Approach</title>
        <abstract>In this paper, we address the problem of combining several <entity id="H01-1058.1">language models
            (LMs)
        </entity> . We find that simple <entity id="H01-1058.2">interpolation methods</entity> , like <entity
                id="H01-1058.3">log-linear and linear interpolation
        </entity> , improve the <entity id="H01-1058.4">performance</entity> but fall short of the <entity
                id="H01-1058.5">performance
        </entity> of an <entity id="H01-1058.6">oracle</entity> . The <entity id="H01-1058.7">oracle</entity> knows the <entity
                id="H01-1058.8">reference word string
        </entity> and selects the <entity id="H01-1058.9">word string</entity> with the best <entity id="H01-1058.10">
            performance
        </entity> (typically, <entity id="H01-1058.11">word or semantic error rate</entity> ) from a list of <entity
                id="H01-1058.12">word strings
        </entity> , where each <entity id="H01-1058.13">word string</entity> has been obtained by using a different <entity
                id="H01-1058.14">LM
        </entity> . Actually, the <entity id="H01-1058.15">oracle</entity> acts like a <entity id="H01-1058.16">dynamic
            combiner
        </entity> with <entity id="H01-1058.17">hard decisions</entity> using the <entity id="H01-1058.18">reference
        </entity> . We provide experimental results that clearly show the need for a <entity id="H01-1058.19">dynamic
            language model combination
        </entity> to improve the <entity id="H01-1058.20">performance</entity> further . We suggest a method that mimics
            the behavior of the <entity id="H01-1058.21">oracle</entity> using a <entity id="H01-1058.22">neural
                network
            </entity> or a <entity id="H01-1058.23">decision tree</entity> . The method amounts to tagging <entity
                    id="H01-1058.24">LMs
            </entity> with <entity id="H01-1058.25">confidence measures</entity> and picking the best <entity
                    id="H01-1058.26">hypothesis
            </entity> corresponding to the <entity id="H01-1058.27">LM</entity> with the best <entity id="H01-1058.28">
                confidence
            </entity> .
        </abstract>


    </text>

    <text id="H01-1070">
        <title>Towards an Intelligent Multilingual Keyboard System</title>
        <abstract>This paper proposes a practical approach employing <entity id="H01-1070.1">n-gram models</entity> and <entity
                id="H01-1070.2">error-correction rules
        </entity> for <entity id="H01-1070.3">Thai key prediction</entity> and <entity id="H01-1070.4">Thai-English
            language identification
        </entity> . The paper also proposes <entity id="H01-1070.5">rule-reduction algorithm</entity> applying <entity
                id="H01-1070.6">mutual information
        </entity> to reduce the <entity id="H01-1070.7">error-correction rules</entity> . Our algorithm reported more
            than 99% <entity id="H01-1070.8">accuracy</entity> in both <entity id="H01-1070.9">language identification
            </entity> and <entity id="H01-1070.10">key prediction</entity> .
        </abstract>


    </text>
</doc>
