"""
This is a script to assign one sentence for each pair of entity
in an abstract
feature vector:
[
shortest dependency path - raw
  shortest dependency path - POS
  distance to main verb/root
]

@author Peace Han
"""
import regex as re
import xml.etree.ElementTree as ET
import replaceEntities
from stanfordcorenlp import StanfordCoreNLP
from pycorenlp import StanfordCoreNLP as scnlp
from shortestDependencyPath import sdp


# This is a class to store text id, title, and original abstract for each document
class Text:
    def __init__(self, text_id, text_title, text_abstract, text_entities):
        self.id = text_id
        self.title = text_title
        self.abstract = text_abstract
        self.sents = split_sents(nlp, text_abstract)
        self.entities = text_entities
        # print("Created Text object {}: {}".format(self.id, self.title))
        # print("\t" + self.abstract)
        # print(self.sents)
        # print(self.entities)

    def __str__(self):
        return "Text {}: {}\n\t{}\n\thas {} sentences and {} entities".format(self.id,
                                                                              self.title,
                                                                              self.abstract,
                                                                              len(self.sents),
                                                                              len(self.entities))


class Pair:
    def __init__(self, ent1, ent2, relation, rev=False):
        self.ent1 = ent1
        self.ent2 = ent2
        self.relation = relation
        self.rev = rev
        assert ent1.split('.')[0] == ent2.split('.')[0]
        self.text_id = ent1.split('.')[0]
        self.sentence = ''
        self.dep_parse = ''

    def __str__(self):
        return "Pair: " \
               "relation: {}; " \
               "{}, {}, REVERSE={}".format(self.relation,
                                           self.ent1,
                                           self.ent2,
                                           self.rev)


def split_sents(nlp, abstract_string):
    # this function takes an nlp object and a full abstract string
    # return: list of sentences
    # sentences are lists of tokens
    sents_list = []  # final list to return
    properties = {'annotators': 'ssplit', 'outputFormat': 'xml'}
    # this actually does the splitting, returns a string in xml format
    ann = nlp.annotate(abstract_string, properties=properties)
    # from the xml string, reconstruct each sentence
    root = ET.fromstring(ann)
    sents_tree = root[0][0]
    for sent in sents_tree:
        out_sent = []
        for tok in sent[0]:
            out_sent.append(tok[0].text)
        sents_list.append(out_sent)
        # sent = ' '.join(out_sent)
        # print(nlp.dependency_parse(sent))
    return sents_list


nlp = StanfordCoreNLP(r'/home/peace/CoreNLP/stanford-corenlp-full-2018-10-05/')

# test = 'This paper shows how J87-3001.1 can be analysed by applying a hierarchy of J87-3001.2 . An experimental system embodying this mechanism has been implemented for processing J87-3001.3 from the J87-3001.4 . A property of this J87-3001.5 , exploited by the system, is that it uses a J87-3001.6 in its J87-3001.7 . The structures generated by the experimental system are intended to be used for the J87-3001.8 of new J87-3001.9 in terms of the J87-3001.10 of J87-3001.11 in the J87-3001.12 . Examples illustrating the output generated are presented, and some qualitative performance results and problems that were encountered are discussed. The analysis process applies successively more specific J87-3001.13 as determined by a hierarchy of J87-3001.14 in which less specific J87-3001.15 dominate more specific ones. This ensures that reasonable incomplete analyses of the J87-3001.16 are produced when more complete analyses are not possible, resulting in a relatively robust J87-3001.17 . Thus the work reported addresses two J87-3001.18 faced by current experimental J87-3001.19 : coping with an incomplete J87-3001.20 and with incomplete J87-3001.21 of J87-3001.22 .'
# print(split_sents(nlp, test))

# takes text.xml data and converts to Text objects
path = 'clean/train_data/'
file = path + '1.1.text.xml'

tree = ET.parse(file)
root = tree.getroot()
print(root)
print(root.tag)

textCount = 0
entCount = 0
texts = {}  # dictionary of textID, Text
for text in root.findall('text'):
    textCount += 1
    attr = text.attrib
    textId = attr.get('id')
    # print(attr)
    # print(text.find('abstract'))
    title = text.find('title').text.strip()  # TODO: further clean up title texts
    abstract = text.find('abstract')
    abstractString = ET.tostring(text.find('abstract'))
    abstractString = abstractString.decode('UTF-8').strip()
    abstractString = ' '.join(abstractString.split('\n'))
    abstractString = ' '.join(re.split('  +', abstractString))
    # [x.strip() for x in abstractString]
    # print("Original abstract: \n\t", abstractString)
    abstractString, entities = replaceEntities.encode(abstractString)
    t = Text(textId, title, abstractString, entities)
    texts[textId] = t
    # sents = split_sents(nlp, abstractString)
print(texts)


file = path + '1.1.relations.txt'
rels = []  # list of Pairs
with open(file) as f:
    for line in f:
        line = line.strip()
        line = line.split('(')
        print(line)
        rel = line[0]
        print(rel)
        ents = line[1][:-1].split(',')
        ent1 = ents[0]
        ent2 = ents[1]
        print(ents)
        print(ent1)
        print(ent2)
        pair = Pair(ent1, ent2, rel, len(ents) == 3)
        pair.relation = rel
        print(pair)
        rels.append(pair)
        # if rel not in rels:
        #     rels[rel] = []
        # rels[rel].append(pair)
print(rels)

# fullList = []  # a master list of all Pairs
print("Assigning sentences for relation type: ", rel)
for pair in rels:
    # print('\t', pair)
    # print(texts[pair.text_id])  # this is the Text object
    textObj = texts[pair.text_id]
    # print(textObj.sents)
    for sent in textObj.sents:
        if (pair.ent1 in sent) and (pair.ent2 in sent):
            pair.sentence = sent
            # print('\t', pair.sentence)
            sent = ' '.join(sent)
            # pair.dep_parse = nlp.dependency_parse(sent)
    # fullList.append(pair)
nlp.close()

# nlp = scnlp('http://localhost:{0}'.format(9000))
outfile = path + '/1.1.features.txt'
entfile = path + '/1.1.text_ents.csv'
o = open(outfile, 'w')
# p = open(entfile, 'w')
print("Generating dependencies...")
for pair in rels:
    print(pair)
    # print(pair.sentence.index(pair.ent1))
    # print(pair.sentence.index(pair.ent2))
    print(pair.sentence)
    pair.dep_parse = sdp(pair)
    pair.dep_parse = replaceEntities.decode(pair.dep_parse, texts[pair.text_id].entities)
    print(pair.dep_parse)
    o.write(pair.dep_parse)
    o.write(' ' + pair.relation)
    # [o.write(x.__str__()) for x in pair.dep_parse]
    o.write('\n')

o.close()

# pair = fullList[0]
# print(pair)
# print(pair.sentence)
# print(pair.dep_parse)


